{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9191649",
   "metadata": {},
   "source": [
    "# MScFE 600 Financial Data - Task 1: Data Quality Analysis\n",
    "\n",
    "**Course**: MScFE 600 Financial Data  \n",
    "**Institution**: WorldQuant University  \n",
    "**Date**: September 2025\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates examples of poor quality financial data, examining both structured and unstructured datasets to understand how they fail to meet data quality standards. The analysis employs KYC (Know Your Customer) data as the primary example for structured data examination, whilst exploring financial news and social media content to illustrate unstructured data quality challenges.\n",
    "\n",
    "The investigation focuses on identifying characteristics of poor quality data across different formats, recognising common issues that compromise data integrity, and understanding the broader implications for financial decision-making processes. Through comprehensive analysis of real-world scenarios, we examine how data quality failures can cascade through financial institutions, affecting everything from regulatory compliance to algorithmic trading systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3801c",
   "metadata": {},
   "source": [
    "## Poor Quality Structured Data: KYC Dataset Analysis\n",
    "\n",
    "Financial institutions rely heavily on structured KYC datasets for regulatory compliance and risk management. These datasets must maintain the highest standards of accuracy, completeness, and consistency to support critical business decisions and meet regulatory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a poor quality KYC dataset with multiple data quality issues\n",
    "def create_poor_kyc_data():\n",
    "    \"\"\"\n",
    "    Creates a KYC dataset with intentional data quality issues to demonstrate\n",
    "    poor data practices in financial institutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Intentionally problematic data\n",
    "    kyc_data = {\n",
    "        'customer_id': [1001, 1002, 1002, 1004, '', 1006, 1007, None, 1009, 1010],  # Duplicates, missing\n",
    "        'first_name': ['John', 'JANE', 'jane', 'Bob', '123Invalid', '', 'Alice', 'Carol', 'Dave', 'Frank'],\n",
    "        'last_name': ['Smith', 'DOE', 'doe', 'Johnson', 'Name456', 'Missing', '', 'Brown', 'Wilson', 'Taylor'],\n",
    "        'date_of_birth': ['1985-01-15', '1990/02/20', '32-12-1988', '1975-13-40', 'Invalid', \n",
    "                         '2025-01-01', '1950-01-01', '', '1980-06-15', '1992-03-10'],\n",
    "        'email': ['john@email.com', 'JANE@GMAIL.COM', 'invalid-email', 'bob@', '', \n",
    "                 'alice@bank.com', 'carol@email', 'duplicate@test.com', 'dave@email.com', 'frank@email.com'],\n",
    "        'phone': ['123-456-7890', '+1-234-567-8901', '12345', '999-999-9999', '', \n",
    "                 '111-111-1111', 'INVALID', '123-456-7890', '+44-20-1234-5678', '555-0123'],\n",
    "        'annual_income': [50000, 150000, -25000, 999999999, 0, None, '', 'High', 75000, 120000],\n",
    "        'country': ['USA', 'usa', 'United States', 'US', '', 'Canada', 'UK', 'UNKNOWN', 'Japan', 'Germany'],\n",
    "        'risk_score': [3.5, 7.2, 15.5, -2.0, None, '', 'Low', 8.9, 4.1, 6.7],  # Out of range values\n",
    "        'kyc_status': ['Verified', 'verified', 'PENDING', 'Rejected', '', 'Unknown', 'verified', 'Pending', 'Approved', 'verified']\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(kyc_data)\n",
    "\n",
    "# Create the problematic dataset\n",
    "poor_kyc_df = create_poor_kyc_data()\n",
    "\n",
    "print(\"Poor Quality KYC Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "print(poor_kyc_df)\n",
    "print(\"\\nDataset Info:\")\n",
    "poor_kyc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42a392",
   "metadata": {},
   "source": [
    "## Analysis of Poor Quality Structured Data\n",
    "\n",
    "The KYC dataset above demonstrates multiple data quality failures that violate fundamental properties required for reliable financial data management.\n",
    "\n",
    "**Accuracy Failures** pervade the dataset through invalid dates such as '32-12-1988' and '1975-13-40' that violate basic calendar constraints. Future birth dates like '2025-01-01' represent logical impossibilities that would trigger immediate validation failures in any robust system. Negative income values of -25000 are unrealistic for KYC purposes and suggest data entry errors or system malfunctions. Risk scores falling outside valid ranges, particularly values like 15.5 and -2.0 when typical scales operate between 1-10, indicate a complete breakdown in data validation controls.\n",
    "\n",
    "**Completeness Problems** manifest through missing customer IDs, the fundamental identifier that enables record linkage and customer identification. Empty strings and null values in critical fields such as names and contact information render customer identification impossible, whilst missing values in regulatory fields like income and risk_score compromise compliance reporting capabilities.\n",
    "\n",
    "**Consistency Violations** appear throughout the dataset with inconsistent formatting for customer names, mixing upper and lower case arbitrarily. Country representations vary wildly, with 'USA', 'usa', 'United States', and 'US' all representing the same entity, creating massive challenges for data aggregation and reporting. Status values exhibit similar inconsistencies with 'Verified', 'verified', and 'PENDING' representing different capitalisations of what should be standardised categories.\n",
    "\n",
    "**Uniqueness Failures** occur when customer_id 1002 appears multiple times, creating fundamental data integrity issues that would corrupt customer relationship management systems and risk assessment calculations.\n",
    "\n",
    "This data fails comprehensively to meet regulatory requirements for KYC compliance, creating significant operational and compliance risks that could result in regulatory penalties, operational failures, and complete breakdown of customer identification processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b587a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment - Quantitative Analysis\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"\n",
    "    Performs comprehensive data quality analysis on the KYC dataset\n",
    "    \"\"\"\n",
    "    print(\"DATA QUALITY ASSESSMENT REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Missing Values Analysis\n",
    "    print(\"\\n1. MISSING VALUES ANALYSIS:\")\n",
    "    missing_stats = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_stats,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # 2. Duplicate Analysis\n",
    "    print(\"\\n2. DUPLICATE ANALYSIS:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Total duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Check for duplicate customer IDs\n",
    "    id_duplicates = df['customer_id'].duplicated().sum()\n",
    "    print(f\"Duplicate customer IDs: {id_duplicates}\")\n",
    "    \n",
    "    # 3. Data Type Issues\n",
    "    print(\"\\n3. DATA TYPE ISSUES:\")\n",
    "    print(\"Current data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # 4. Inconsistency Analysis\n",
    "    print(\"\\n4. INCONSISTENCY ANALYSIS:\")\n",
    "    \n",
    "    # Country field inconsistencies\n",
    "    country_values = df['country'].value_counts()\n",
    "    print(f\"Country field unique values: {len(country_values)}\")\n",
    "    print(\"Country variations:\", country_values.index.tolist())\n",
    "    \n",
    "    # Status field inconsistencies  \n",
    "    status_values = df['kyc_status'].value_counts()\n",
    "    print(f\"Status field unique values: {len(status_values)}\")\n",
    "    print(\"Status variations:\", status_values.index.tolist())\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Run the analysis\n",
    "quality_report = analyze_data_quality(poor_kyc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87d238",
   "metadata": {},
   "source": [
    "## Poor Quality Unstructured Data: Financial News and Social Media\n",
    "\n",
    "Unstructured financial data presents unique challenges for quality assessment, particularly when sourcing information from diverse channels including traditional financial media, social platforms, and user-generated content for sentiment analysis applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create poor quality unstructured financial news data\n",
    "def create_poor_financial_news_data():\n",
    "    \"\"\"\n",
    "    Creates financial news/social media dataset with quality issues\n",
    "    for sentiment analysis applications\n",
    "    \"\"\"\n",
    "    \n",
    "    poor_news_data = [\n",
    "        {\n",
    "            'source': 'Twitter',\n",
    "            'timestamp': '2024-15-45 25:99:99',  # Invalid timestamp\n",
    "            'content': 'AAPL to the moon!!! 🚀🚀🚀 #stocks #yolo BUY BUY BUY!!!',\n",
    "            'author': '@anonymous123',\n",
    "            'sentiment_label': 'VERY_POSITIVE'\n",
    "        },\n",
    "        {\n",
    "            'source': 'Bloomberg',\n",
    "            'timestamp': '',  # Missing timestamp\n",
    "            'content': 'The Federal Reserve announced... [CONTENT TRUNCATED] ...rate decisions will impact market volatility significantly.',\n",
    "            'author': 'Unknown Author',\n",
    "            'sentiment_label': None\n",
    "        },\n",
    "        {\n",
    "            'source': 'Reddit',\n",
    "            'timestamp': 'yesterday',  # Ambiguous timestamp\n",
    "            'content': 'Tesla stock is going to crash because Elon Musk tweeted something about aliens 👽. My neighbor told me he heard from his friend that works at Tesla that they are going to announce bankruptcy next week. This is not financial advice but you should probably sell everything now!!!',\n",
    "            'author': 'deleted_user',\n",
    "            'sentiment_label': 'negative'\n",
    "        },\n",
    "        {\n",
    "            'source': 'WSJ',\n",
    "            'timestamp': '2024-09-30T14:30:00Z',\n",
    "            'content': '<html><body><div class=\"article\">Market analysis shows that Q3 earnings for tech sector have exceeded expectations by 15% on average. However, concerns about inflation persist among investors.</div></body></html>',\n",
    "            'author': 'Jane Smith, Financial Analyst',\n",
    "            'sentiment_label': 'NEUTRAL'\n",
    "        },\n",
    "        {\n",
    "            'source': 'Discord',\n",
    "            'timestamp': '2024-09-30T15:45:00Z',\n",
    "            'content': 'guys i put my entire life savings into crypto and now im broke 😭 dont do what i did my wife left me and took the kids',\n",
    "            'author': 'crypto_king_2024',\n",
    "            'sentiment_label': 'EXTREMELY_NEGATIVE'\n",
    "        },\n",
    "        {\n",
    "            'source': 'Financial Times',\n",
    "            'timestamp': '2024-09-30T16:00:00Z',\n",
    "            'content': 'The S&P 500 index closed higher today, driven by strong performance in the technology sector. Analysts remain cautiously optimistic about Q4 prospects despite ongoing geopolitical tensions.',\n",
    "            'author': 'Dr. Michael Chen, Senior Market Strategist',\n",
    "            'sentiment_label': 'positive'\n",
    "        },\n",
    "        {\n",
    "            'source': 'TikTok',\n",
    "            'timestamp': '2024-09-30T17:15:00Z',\n",
    "            'content': 'OMG guys!! I just discovered this AMAZING trading strategy that made me $1000 in 5 minutes!!! Link in bio!!! #daytrading #getrich #financialfreedom #notascam',\n",
    "            'author': '@trading_guru_18',\n",
    "            'sentiment_label': '🚀🚀🚀'\n",
    "        },\n",
    "        {\n",
    "            'source': '',  # Missing source\n",
    "            'timestamp': '2024-09-30T18:00:00Z',\n",
    "            'content': '',  # Empty content\n",
    "            'author': '',\n",
    "            'sentiment_label': ''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return poor_news_data\n",
    "\n",
    "# Create and display the poor quality unstructured data\n",
    "poor_news = create_poor_financial_news_data()\n",
    "\n",
    "print(\"Poor Quality Financial News/Social Media Dataset:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, article in enumerate(poor_news, 1):\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Source: {article['source']}\")\n",
    "    print(f\"Timestamp: {article['timestamp']}\")\n",
    "    print(f\"Author: {article['author']}\")\n",
    "    print(f\"Content: {article['content'][:100]}{'...' if len(article['content']) > 100 else ''}\")\n",
    "    print(f\"Sentiment: {article['sentiment_label']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f70257",
   "metadata": {},
   "source": [
    "## Analysis of Poor Quality Unstructured Data\n",
    "\n",
    "The financial news and social media dataset demonstrates critical failures in unstructured data quality that would severely compromise sentiment analysis and algorithmic trading decisions.\n",
    "\n",
    "**Reliability and Source Credibility Issues** emerge from mixing highly credible sources such as Bloomberg, WSJ, and Financial Times with unreliable social media accounts including anonymous Twitter users, TikTok influencers, and deleted Reddit accounts. This creates a false equivalency where unreliable speculation carries the same analytical weight as professional financial analysis, leading to skewed sentiment scores and potentially catastrophic trading decisions. The credibility spectrum ranges from verified financial journalists with established track records to anonymous accounts promoting questionable trading strategies.\n",
    "\n",
    "**Temporal Inconsistency and Context Loss** manifest through missing, invalid, or ambiguous timestamp information. Critical temporal data appears as impossible timestamps like '2024-15-45 25:99:99', completely missing timestamp fields, or vague references such as 'yesterday' that provide no actionable timing information. Without precise timestamps, the data cannot support time-sensitive financial applications where correlation with market events and establishment of causal relationships becomes impossible.\n",
    "\n",
    "**Content Integrity and Processing Challenges** appear throughout the dataset with truncated articles missing crucial context, HTML markup contaminating text content, and emoji symbols replacing standardised sentiment classifications. Empty records provide no informational value whilst creating processing overhead. This inconsistency makes automated processing extremely difficult and prone to errors, as sentiment analysis algorithms cannot reliably extract meaningful signals from corrupted or incomplete text data.\n",
    "\n",
    "**Standardisation and Classification Failures** occur when sentiment labels employ inconsistent formats including 'VERY_POSITIVE', 'negative', '🚀🚀🚀', and empty values that cannot be systematically processed or compared. This lack of standardisation means the data cannot feed into machine learning models or quantitative analysis pipelines, rendering the entire dataset unusable for its intended purpose in financial decision-making systems. The absence of controlled vocabularies and classification schemes makes cross-platform analysis impossible whilst preventing meaningful aggregation of sentiment indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35522410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Data Quality Assessment\n",
    "def analyze_unstructured_data_quality(news_data):\n",
    "    \"\"\"\n",
    "    Analyzes quality issues in unstructured financial news data\n",
    "    \"\"\"\n",
    "    print(\"UNSTRUCTURED DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_articles = len(news_data)\n",
    "    \n",
    "    # 1. Missing/Empty Content Analysis\n",
    "    empty_content = sum(1 for article in news_data if not article['content'].strip())\n",
    "    empty_source = sum(1 for article in news_data if not article['source'].strip())\n",
    "    empty_timestamp = sum(1 for article in news_data if not article['timestamp'].strip())\n",
    "    empty_author = sum(1 for article in news_data if not article['author'].strip())\n",
    "    \n",
    "    print(f\"\\n1. COMPLETENESS ANALYSIS:\")\n",
    "    print(f\"Total articles: {total_articles}\")\n",
    "    print(f\"Empty content: {empty_content} ({empty_content/total_articles*100:.1f}%)\")\n",
    "    print(f\"Missing source: {empty_source} ({empty_source/total_articles*100:.1f}%)\")\n",
    "    print(f\"Missing timestamp: {empty_timestamp} ({empty_timestamp/total_articles*100:.1f}%)\")\n",
    "    print(f\"Missing author: {empty_author} ({empty_author/total_articles*100:.1f}%)\")\n",
    "    \n",
    "    # 2. Source Reliability Analysis\n",
    "    sources = [article['source'] for article in news_data if article['source']]\n",
    "    reliable_sources = ['Bloomberg', 'WSJ', 'Financial Times', 'Reuters']\n",
    "    unreliable_sources = ['Twitter', 'Reddit', 'TikTok', 'Discord']\n",
    "    \n",
    "    reliable_count = sum(1 for source in sources if source in reliable_sources)\n",
    "    unreliable_count = sum(1 for source in sources if source in unreliable_sources)\n",
    "    \n",
    "    print(f\"\\n2. SOURCE RELIABILITY ANALYSIS:\")\n",
    "    print(f\"Reliable sources: {reliable_count} ({reliable_count/len(sources)*100:.1f}%)\")\n",
    "    print(f\"Unreliable sources: {unreliable_count} ({unreliable_count/len(sources)*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Content Quality Issues\n",
    "    html_contaminated = sum(1 for article in news_data if '<html>' in article['content'] or '<div>' in article['content'])\n",
    "    emoji_heavy = sum(1 for article in news_data if '🚀' in article['content'] or '😭' in article['content'] or '👽' in article['content'])\n",
    "    truncated = sum(1 for article in news_data if '[CONTENT TRUNCATED]' in article['content'])\n",
    "    \n",
    "    print(f\"\\n3. CONTENT QUALITY ISSUES:\")\n",
    "    print(f\"HTML contaminated: {html_contaminated} ({html_contaminated/total_articles*100:.1f}%)\")\n",
    "    print(f\"Emoji-heavy content: {emoji_heavy} ({emoji_heavy/total_articles*100:.1f}%)\")\n",
    "    print(f\"Truncated content: {truncated} ({truncated/total_articles*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Sentiment Label Consistency\n",
    "    sentiment_labels = [article['sentiment_label'] for article in news_data if article['sentiment_label']]\n",
    "    unique_formats = set(sentiment_labels)\n",
    "    \n",
    "    print(f\"\\n4. SENTIMENT LABEL ANALYSIS:\")\n",
    "    print(f\"Unique sentiment formats: {len(unique_formats)}\")\n",
    "    print(f\"Sentiment label formats: {list(unique_formats)}\")\n",
    "    print(f\"Standardization level: {'Poor - Multiple inconsistent formats' if len(unique_formats) > 3 else 'Good'}\")\n",
    "\n",
    "# Run the unstructured data analysis\n",
    "analyze_unstructured_data_quality(poor_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3018b5",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Structured data quality in the KYC dataset revealed how fundamental data management failures can compromise regulatory compliance and operational effectiveness in financial institutions. The analysis demonstrated that missing values, duplicate records, inconsistent formatting, and invalid data ranges create cascading problems throughout customer relationship management and risk assessment systems.\n",
    "\n",
    "Unstructured data quality challenges in financial news and social media present unique difficulties including source reliability assessment, content contamination, temporal inconsistencies, and standardisation issues. These problems can lead to flawed sentiment analysis, poor trading decisions, and systematic misinterpretation of market signals.\n",
    "\n",
    "The broader implications for financial data quality extend beyond immediate operational concerns to encompass regulatory compliance failures, inaccurate risk assessments, compromised algorithmic trading decisions, and potential loss of customer trust. Effective data quality management requires robust validation rules, clear governance policies, reliable source verification, complete audit trails, and continuous monitoring procedures.\n",
    "\n",
    "Organisations must implement comprehensive data quality frameworks that address both structured and unstructured data challenges whilst maintaining the flexibility to adapt to evolving data sources and analytical requirements. The cost of poor data quality in financial services extends far beyond immediate operational inconvenience to encompass significant regulatory, reputational, and financial risks.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Wang, R. Y., & Strong, D. M. (1996). Beyond accuracy: What data quality means to data consumers. *Journal of Management Information Systems*, 12(4), 5-33.\n",
    "- Batini, C., & Scannapieco, M. (2016). *Data and information quality: Dimensions, principles and techniques*. Springer.\n",
    "- Financial Conduct Authority. (2021). *Data governance and quality standards for financial institutions*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
