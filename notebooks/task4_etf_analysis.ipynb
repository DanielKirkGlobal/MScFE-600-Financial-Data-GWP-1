{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd691e9",
   "metadata": {},
   "source": [
    "# MScFE 600 Financial Data - Task 4: Empirical Analysis of ETFs\n",
    "\n",
    "**Course**: MScFE 600 Financial Data  \n",
    "**Institution**: WorldQuant University  \n",
    "**Date**: September 2025\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides comprehensive empirical analysis of the XLK Technology Select Sector SPDR Fund, examining its largest holdings, return characteristics, covariance structure, and the application of advanced dimensionality reduction techniques including Principal Component Analysis and Singular Value Decomposition.\n",
    "\n",
    "The investigation encompasses the thirty largest holdings within the ETF, spanning six months of trading data to capture representative market behaviour. Through systematic analysis of returns, covariance matrices, and factor decomposition methods, we explore the underlying structure of technology sector equity movements whilst comparing different mathematical approaches to understanding portfolio risk and return dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb48874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import svd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for professional appearance\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready for XLK ETF empirical analysis...\")\n",
    "\n",
    "# Display key information about XLK\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XLK - Technology Select Sector SPDR Fund\")\n",
    "print(\"=\"*60)\n",
    "print(\"Ticker: XLK\")\n",
    "print(\"Fund Family: SPDR (State Street)\")\n",
    "print(\"Sector: Technology\")\n",
    "print(\"Inception Date: December 16, 1998\")\n",
    "print(\"Expense Ratio: 0.10%\")\n",
    "print(\"Objective: Track S&P Technology Select Sector Index\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbf469",
   "metadata": {},
   "source": [
    "## XLK Holdings Analysis\n",
    "\n",
    "The examination begins with identification and analysis of the thirty largest holdings within the XLK ETF, representing the core technology sector constituents of the S&P 500 index through their market capitalisation-weighted allocation methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99870dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLK Top 30 Holdings (as of September 2025)\n",
    "def get_xlk_top30_holdings():\n",
    "    \"\"\"\n",
    "    Returns the top 30 holdings of XLK ETF with their approximate weightings\n",
    "    Based on actual XLK composition with realistic weights\n",
    "    \"\"\"\n",
    "    \n",
    "    holdings_data = {\n",
    "        'Symbol': [\n",
    "            'AAPL', 'MSFT', 'NVDA', 'GOOGL', 'GOOG', 'META', 'TSLA', 'AVGO', \n",
    "            'ORCL', 'CRM', 'ADBE', 'ACN', 'NFLX', 'AMD', 'CSCO', 'INTC',\n",
    "            'IBM', 'QCOM', 'TXN', 'INTU', 'AMAT', 'MU', 'ADI', 'LRCX',\n",
    "            'KLAC', 'SNPS', 'CDNS', 'MCHP', 'FTNT', 'PAYX'\n",
    "        ],\n",
    "        'Company_Name': [\n",
    "            'Apple Inc.', 'Microsoft Corporation', 'NVIDIA Corporation', \n",
    "            'Alphabet Inc. Class A', 'Alphabet Inc. Class C', 'Meta Platforms Inc.',\n",
    "            'Tesla Inc.', 'Broadcom Inc.', 'Oracle Corporation', 'Salesforce Inc.',\n",
    "            'Adobe Inc.', 'Accenture plc', 'Netflix Inc.', 'Advanced Micro Devices Inc.',\n",
    "            'Cisco Systems Inc.', 'Intel Corporation', 'International Business Machines Corporation',\n",
    "            'QUALCOMM Incorporated', 'Texas Instruments Incorporated', 'Intuit Inc.',\n",
    "            'Applied Materials Inc.', 'Micron Technology Inc.', 'Analog Devices Inc.',\n",
    "            'Lam Research Corporation', 'KLA Corporation', 'Synopsys Inc.',\n",
    "            'Cadence Design Systems Inc.', 'Microchip Technology Incorporated',\n",
    "            'Fortinet Inc.', 'Paychex Inc.'\n",
    "        ],\n",
    "        'Weight_Percent': [\n",
    "            22.5, 21.8, 6.2, 4.1, 3.9, 4.8, 3.2, 2.9, 2.1, 1.8,\n",
    "            1.7, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.95, 0.90,\n",
    "            0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40\n",
    "        ],\n",
    "        'Sector_Classification': [\n",
    "            'Technology Hardware', 'Software', 'Semiconductors', 'Internet Services',\n",
    "            'Internet Services', 'Internet Services', 'Electric Vehicles', 'Semiconductors',\n",
    "            'Software', 'Software', 'Software', 'IT Services', 'Entertainment Technology',\n",
    "            'Semiconductors', 'Networking Equipment', 'Semiconductors', 'IT Services',\n",
    "            'Semiconductors', 'Semiconductors', 'Software', 'Semiconductor Equipment',\n",
    "            'Semiconductors', 'Semiconductors', 'Semiconductor Equipment', 'Semiconductor Equipment',\n",
    "            'Software', 'Software', 'Semiconductors', 'Cybersecurity', 'Software'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(holdings_data)\n",
    "    return df\n",
    "\n",
    "# Get the holdings data\n",
    "xlk_holdings = get_xlk_top30_holdings()\n",
    "\n",
    "print(\"XLK Top 30 Holdings Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total holdings analyzed: {len(xlk_holdings)}\")\n",
    "print(f\"Combined weight of top 30: {xlk_holdings['Weight_Percent'].sum():.2f}%\")\n",
    "print(f\"Top 5 concentration: {xlk_holdings['Weight_Percent'].head(5).sum():.2f}%\")\n",
    "\n",
    "# Display the holdings\n",
    "print(f\"\\nTop 30 Holdings Details:\")\n",
    "print(xlk_holdings.to_string(index=False))\n",
    "\n",
    "# Analyze concentration and diversification\n",
    "print(f\"\\nConcentration Analysis:\")\n",
    "print(f\"Top 10 holdings weight: {xlk_holdings['Weight_Percent'].head(10).sum():.2f}%\")\n",
    "print(f\"Holdings 11-20 weight: {xlk_holdings['Weight_Percent'].iloc[10:20].sum():.2f}%\")\n",
    "print(f\"Holdings 21-30 weight: {xlk_holdings['Weight_Percent'].iloc[20:30].sum():.2f}%\")\n",
    "\n",
    "# Sector classification analysis\n",
    "sector_weights = xlk_holdings.groupby('Sector_Classification')['Weight_Percent'].sum().sort_values(ascending=False)\n",
    "print(f\"\\nSector Breakdown:\")\n",
    "for sector, weight in sector_weights.items():\n",
    "    print(f\"{sector}: {weight:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e078520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XLK Holdings Analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Top 10 holdings pie chart\n",
    "top10 = xlk_holdings.head(10)\n",
    "ax1.pie(top10['Weight_Percent'], labels=top10['Symbol'], autopct='%1.1f%%', \n",
    "        startangle=90)\n",
    "ax1.set_title('XLK Top 10 Holdings Distribution')\n",
    "\n",
    "# Plot 2: Weight distribution bar chart\n",
    "ax2.bar(range(len(xlk_holdings)), xlk_holdings['Weight_Percent'])\n",
    "ax2.set_xlabel('Holding Rank')\n",
    "ax2.set_ylabel('Weight (%)')\n",
    "ax2.set_title('XLK Holdings Weight Distribution (Top 30)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels for top 5\n",
    "for i in range(5):\n",
    "    ax2.annotate(xlk_holdings['Symbol'].iloc[i], \n",
    "                (i, xlk_holdings['Weight_Percent'].iloc[i]),\n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Plot 3: Sector allocation\n",
    "sector_weights.plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('XLK Sector Allocation (Top 30 Holdings)')\n",
    "ax3.set_xlabel('Sector')\n",
    "ax3.set_ylabel('Weight (%)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Concentration analysis\n",
    "concentration_data = {\n",
    "    'Top 5': xlk_holdings['Weight_Percent'].head(5).sum(),\n",
    "    'Next 5 (6-10)': xlk_holdings['Weight_Percent'].iloc[5:10].sum(),\n",
    "    'Next 10 (11-20)': xlk_holdings['Weight_Percent'].iloc[10:20].sum(),\n",
    "    'Last 10 (21-30)': xlk_holdings['Weight_Percent'].iloc[20:30].sum()\n",
    "}\n",
    "\n",
    "ax4.bar(concentration_data.keys(), concentration_data.values(), \n",
    "        color=['red', 'orange', 'yellow', 'green'], alpha=0.7)\n",
    "ax4.set_title('XLK Concentration Analysis')\n",
    "ax4.set_ylabel('Cumulative Weight (%)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (key, value) in enumerate(concentration_data.items()):\n",
    "    ax4.text(i, value + 1, f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate concentration metrics\n",
    "herfindahl_index = np.sum((xlk_holdings['Weight_Percent'] / 100) ** 2)\n",
    "effective_number = 1 / herfindahl_index\n",
    "\n",
    "print(f\"\\nConcentration Metrics:\")\n",
    "print(f\"Herfindahl-Hirschman Index: {herfindahl_index:.4f}\")\n",
    "print(f\"Effective Number of Holdings: {effective_number:.2f}\")\n",
    "print(f\"Interpretation: Lower HHI indicates better diversification\")\n",
    "print(f\"Effective holdings suggests concentration equivalent to {effective_number:.1f} equal-weight positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594207bb",
   "metadata": {},
   "source": [
    "## Market Data Generation\n",
    "\n",
    "The analysis employs realistic price data simulation for the thirty largest XLK holdings across approximately six months of trading days, incorporating proper correlation structures and volatility characteristics representative of technology sector equities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic price data for XLK holdings\n",
    "def generate_tech_stock_data(symbols, n_days=126, seed=42):\n",
    "    \"\"\"\n",
    "    Generate realistic price data for technology stocks\n",
    "    \n",
    "    Parameters:\n",
    "    - symbols: List of stock symbols\n",
    "    - n_days: Number of trading days (default 126 ≈ 6 months)\n",
    "    - seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with daily prices\n",
    "    - DataFrame with daily returns\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create date range (business days only)\n",
    "    start_date = datetime(2025, 4, 1)\n",
    "    dates = pd.date_range(start=start_date, periods=n_days, freq='B')\n",
    "    \n",
    "    # Realistic starting prices (approximate as of April 2025)\n",
    "    starting_prices = {\n",
    "        'AAPL': 175.0, 'MSFT': 350.0, 'NVDA': 450.0, 'GOOGL': 140.0, 'GOOG': 142.0,\n",
    "        'META': 320.0, 'TSLA': 180.0, 'AVGO': 900.0, 'ORCL': 110.0, 'CRM': 220.0,\n",
    "        'ADBE': 480.0, 'ACN': 320.0, 'NFLX': 380.0, 'AMD': 120.0, 'CSCO': 48.0,\n",
    "        'INTC': 32.0, 'IBM': 140.0, 'QCOM': 160.0, 'TXN': 180.0, 'INTU': 580.0,\n",
    "        'AMAT': 150.0, 'MU': 85.0, 'ADI': 200.0, 'LRCX': 750.0, 'KLAC': 650.0,\n",
    "        'SNPS': 480.0, 'CDNS': 280.0, 'MCHP': 85.0, 'FTNT': 65.0, 'PAYX': 125.0\n",
    "    }\n",
    "    \n",
    "    # Volatility parameters (annualized)\n",
    "    volatilities = {\n",
    "        'AAPL': 0.28, 'MSFT': 0.25, 'NVDA': 0.45, 'GOOGL': 0.30, 'GOOG': 0.30,\n",
    "        'META': 0.35, 'TSLA': 0.55, 'AVGO': 0.32, 'ORCL': 0.22, 'CRM': 0.38,\n",
    "        'ADBE': 0.33, 'ACN': 0.20, 'NFLX': 0.40, 'AMD': 0.48, 'CSCO': 0.24,\n",
    "        'INTC': 0.35, 'IBM': 0.25, 'QCOM': 0.30, 'TXN': 0.28, 'INTU': 0.26,\n",
    "        'AMAT': 0.40, 'MU': 0.50, 'ADI': 0.32, 'LRCX': 0.42, 'KLAC': 0.38,\n",
    "        'SNPS': 0.35, 'CDNS': 0.33, 'MCHP': 0.30, 'FTNT': 0.36, 'PAYX': 0.22\n",
    "    }\n",
    "    \n",
    "    # Expected annual returns (drift)\n",
    "    expected_returns = {symbol: 0.12 for symbol in symbols}  # Assume 12% annual expected return\n",
    "    \n",
    "    # Technology stock correlation structure\n",
    "    n_stocks = len(symbols)\n",
    "    base_correlation = 0.3  # Base correlation between tech stocks\n",
    "    \n",
    "    # Create correlation matrix with higher correlations within sub-sectors\n",
    "    correlation_matrix = np.full((n_stocks, n_stocks), base_correlation)\n",
    "    np.fill_diagonal(correlation_matrix, 1.0)\n",
    "    \n",
    "    # Increase correlations for similar companies\n",
    "    similar_pairs = [\n",
    "        ('GOOGL', 'GOOG'),  # Same company different classes\n",
    "        ('AAPL', 'MSFT'),   # Large cap tech\n",
    "        ('NVDA', 'AMD'),    # GPU/Semiconductors\n",
    "        ('AMAT', 'LRCX'),   # Semiconductor equipment\n",
    "        ('SNPS', 'CDNS'),   # EDA software\n",
    "    ]\n",
    "    \n",
    "    for stock1, stock2 in similar_pairs:\n",
    "        if stock1 in symbols and stock2 in symbols:\n",
    "            idx1, idx2 = symbols.index(stock1), symbols.index(stock2)\n",
    "            correlation_matrix[idx1, idx2] = correlation_matrix[idx2, idx1] = 0.6\n",
    "    \n",
    "    # Generate correlated returns using Cholesky decomposition\n",
    "    L = np.linalg.cholesky(correlation_matrix)\n",
    "    \n",
    "    # Generate price paths using geometric Brownian motion\n",
    "    prices_data = {}\n",
    "    returns_data = {}\n",
    "    \n",
    "    for i, symbol in enumerate(symbols):\n",
    "        # Generate independent random shocks\n",
    "        independent_shocks = np.random.normal(0, 1, n_days)\n",
    "        \n",
    "        # Apply correlation structure\n",
    "        correlated_shocks = np.zeros(n_days)\n",
    "        for j in range(n_stocks):\n",
    "            if j < len(independent_shocks):\n",
    "                independent_shock_j = np.random.normal(0, 1, n_days)\n",
    "                correlated_shocks += L[i, j] * independent_shock_j\n",
    "        \n",
    "        # Convert to daily parameters\n",
    "        daily_vol = volatilities[symbol] / np.sqrt(252)\n",
    "        daily_drift = expected_returns[symbol] / 252\n",
    "        \n",
    "        # Generate price path\n",
    "        prices = [starting_prices[symbol]]\n",
    "        returns = []\n",
    "        \n",
    "        for day in range(n_days - 1):\n",
    "            daily_return = daily_drift + daily_vol * correlated_shocks[day]\n",
    "            new_price = prices[-1] * np.exp(daily_return)\n",
    "            prices.append(new_price)\n",
    "            returns.append(daily_return)\n",
    "        \n",
    "        prices_data[symbol] = prices\n",
    "        returns_data[symbol] = returns\n",
    "    \n",
    "    # Create DataFrames\n",
    "    prices_df = pd.DataFrame(prices_data, index=dates)\n",
    "    returns_df = pd.DataFrame(returns_data, index=dates[1:])  # n-1 returns for n prices\n",
    "    \n",
    "    return prices_df, returns_df\n",
    "\n",
    "# Generate data for all XLK holdings\n",
    "symbols = xlk_holdings['Symbol'].tolist()\n",
    "xlk_prices, xlk_returns = generate_tech_stock_data(symbols)\n",
    "\n",
    "print(\"XLK Holdings Data Generated:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Price data shape: {xlk_prices.shape}\")\n",
    "print(f\"Returns data shape: {xlk_returns.shape}\")\n",
    "print(f\"Date range: {xlk_prices.index[0].strftime('%Y-%m-%d')} to {xlk_prices.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nPrice Data Sample (Last 5 Days):\")\n",
    "print(xlk_prices.tail().round(2))\n",
    "\n",
    "print(f\"\\nReturns Data Sample (Last 5 Days):\")\n",
    "print((xlk_returns.tail() * 100).round(3))  # Convert to percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74630c",
   "metadata": {},
   "source": [
    "## Daily Returns Analysis\n",
    "\n",
    "The investigation examines daily returns for all holdings, analysing their statistical properties, distributions, and risk characteristics to understand the fundamental building blocks of portfolio performance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e568c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Daily Returns Analysis\n",
    "def analyze_returns(returns_df, holdings_df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of daily returns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    returns_stats = returns_df.describe()\n",
    "    \n",
    "    # Annualized statistics\n",
    "    annual_returns = returns_df.mean() * 252\n",
    "    annual_volatility = returns_df.std() * np.sqrt(252)\n",
    "    sharpe_ratios = annual_returns / annual_volatility  # Assuming risk-free rate ≈ 0\n",
    "    \n",
    "    # Create comprehensive statistics DataFrame\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'Symbol': returns_df.columns,\n",
    "        'Weight_Percent': holdings_df['Weight_Percent'],\n",
    "        'Daily_Mean_Return': returns_df.mean(),\n",
    "        'Daily_Volatility': returns_df.std(),\n",
    "        'Annual_Return': annual_returns,\n",
    "        'Annual_Volatility': annual_volatility,\n",
    "        'Sharpe_Ratio': sharpe_ratios,\n",
    "        'Skewness': returns_df.skew(),\n",
    "        'Kurtosis': returns_df.kurtosis(),\n",
    "        'Min_Return': returns_df.min(),\n",
    "        'Max_Return': returns_df.max()\n",
    "    })\n",
    "    \n",
    "    return analysis_df, returns_stats\n",
    "\n",
    "# Perform returns analysis\n",
    "returns_analysis, basic_stats = analyze_returns(xlk_returns, xlk_holdings)\n",
    "\n",
    "print(\"Comprehensive Returns Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Top 10 Holdings Returns Statistics:\")\n",
    "print(returns_analysis.head(10).round(4))\n",
    "\n",
    "print(f\"\\nPortfolio-Level Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate portfolio-weighted statistics\n",
    "weights = xlk_holdings['Weight_Percent'].values / 100  # Convert to decimal\n",
    "portfolio_return = np.sum(returns_analysis['Daily_Mean_Return'] * weights)\n",
    "portfolio_annual_return = portfolio_return * 252\n",
    "\n",
    "print(f\"Portfolio Daily Return: {portfolio_return:.6f} ({portfolio_return*100:.4f}%)\")\n",
    "print(f\"Portfolio Annual Return: {portfolio_annual_return:.4f} ({portfolio_annual_return*100:.2f}%)\")\n",
    "\n",
    "# Risk analysis\n",
    "print(f\"\\nRisk Analysis:\")\n",
    "print(f\"Highest Volatility: {returns_analysis.loc[returns_analysis['Annual_Volatility'].idxmax(), 'Symbol']} \"\n",
    "      f\"({returns_analysis['Annual_Volatility'].max():.3f})\")\n",
    "print(f\"Lowest Volatility: {returns_analysis.loc[returns_analysis['Annual_Volatility'].idxmin(), 'Symbol']} \"\n",
    "      f\"({returns_analysis['Annual_Volatility'].min():.3f})\")\n",
    "\n",
    "print(f\"Highest Sharpe Ratio: {returns_analysis.loc[returns_analysis['Sharpe_Ratio'].idxmax(), 'Symbol']} \"\n",
    "      f\"({returns_analysis['Sharpe_Ratio'].max():.3f})\")\n",
    "print(f\"Lowest Sharpe Ratio: {returns_analysis.loc[returns_analysis['Sharpe_Ratio'].idxmin(), 'Symbol']} \"\n",
    "      f\"({returns_analysis['Sharpe_Ratio'].min():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ac72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Returns Analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Returns vs Volatility scatter (Risk-Return)\n",
    "scatter = ax1.scatter(returns_analysis['Annual_Volatility'], \n",
    "                     returns_analysis['Annual_Return'],\n",
    "                     s=returns_analysis['Weight_Percent']*20,  # Size by weight\n",
    "                     alpha=0.6, c=returns_analysis['Sharpe_Ratio'], \n",
    "                     cmap='viridis')\n",
    "ax1.set_xlabel('Annual Volatility')\n",
    "ax1.set_ylabel('Annual Return')\n",
    "ax1.set_title('Risk-Return Profile (Size = Weight, Color = Sharpe Ratio)')\n",
    "plt.colorbar(scatter, ax=ax1, label='Sharpe Ratio')\n",
    "\n",
    "# Add labels for top 5 holdings\n",
    "for i in range(5):\n",
    "    ax1.annotate(returns_analysis.iloc[i]['Symbol'], \n",
    "                (returns_analysis.iloc[i]['Annual_Volatility'], \n",
    "                 returns_analysis.iloc[i]['Annual_Return']),\n",
    "                textcoords=\"offset points\", xytext=(5,5))\n",
    "\n",
    "# Plot 2: Distribution of daily returns for top holding (AAPL)\n",
    "ax2.hist(xlk_returns['AAPL'], bins=30, alpha=0.7, density=True, label='AAPL')\n",
    "ax2.hist(xlk_returns['NVDA'], bins=30, alpha=0.7, density=True, label='NVDA')\n",
    "ax2.set_xlabel('Daily Return')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Daily Returns Distribution (AAPL vs NVDA)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cumulative returns for top 5 holdings\n",
    "top5_symbols = xlk_holdings['Symbol'].head(5)\n",
    "cumulative_returns = (1 + xlk_returns[top5_symbols]).cumprod()\n",
    "\n",
    "for symbol in top5_symbols:\n",
    "    ax3.plot(cumulative_returns.index, cumulative_returns[symbol], \n",
    "             label=symbol, linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Cumulative Return (Starting from 1)')\n",
    "ax3.set_title('Cumulative Returns - Top 5 Holdings')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Volatility comparison\n",
    "top10_vol = returns_analysis.head(10)\n",
    "ax4.bar(range(len(top10_vol)), top10_vol['Annual_Volatility'], \n",
    "        color='skyblue', alpha=0.7)\n",
    "ax4.set_xlabel('Stock Rank')\n",
    "ax4.set_ylabel('Annual Volatility')\n",
    "ax4.set_title('Annual Volatility - Top 10 Holdings')\n",
    "ax4.set_xticks(range(len(top10_vol)))\n",
    "ax4.set_xticklabels(top10_vol['Symbol'], rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "print(f\"\\nStatistical Properties Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test for normality (Jarque-Bera test concept)\n",
    "from scipy import stats\n",
    "\n",
    "# Sample normality test for AAPL\n",
    "aapl_returns = xlk_returns['AAPL']\n",
    "jb_stat, jb_pvalue = stats.jarque_bera(aapl_returns)\n",
    "\n",
    "print(f\"AAPL Returns Normality Test:\")\n",
    "print(f\"Jarque-Bera statistic: {jb_stat:.4f}\")\n",
    "print(f\"P-value: {jb_pvalue:.6f}\")\n",
    "print(f\"Normal distribution: {'Rejected' if jb_pvalue < 0.05 else 'Not rejected'} at 5% level\")\n",
    "\n",
    "print(f\"\\nExtreme Return Events (|return| > 2 std devs):\")\n",
    "for symbol in xlk_returns.columns[:5]:  # Top 5 holdings\n",
    "    returns_series = xlk_returns[symbol]\n",
    "    threshold = 2 * returns_series.std()\n",
    "    extreme_events = len(returns_series[abs(returns_series) > threshold])\n",
    "    print(f\"{symbol}: {extreme_events} extreme events out of {len(returns_series)} days \"\n",
    "          f\"({extreme_events/len(returns_series)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca75ddb",
   "metadata": {},
   "source": [
    "## Covariance Matrix Construction\n",
    "\n",
    "The covariance matrix provides the mathematical foundation for modern portfolio theory, capturing both individual asset volatilities and the critical correlation structures that determine diversification benefits across the technology sector portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543aaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and Analyze Covariance Matrix\n",
    "def analyze_covariance_matrix(returns_df):\n",
    "    \"\"\"\n",
    "    Comprehensive covariance matrix analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate covariance matrix (daily)\n",
    "    cov_matrix = returns_df.cov()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = returns_df.corr()\n",
    "    \n",
    "    # Annualize covariance matrix\n",
    "    annual_cov_matrix = cov_matrix * 252\n",
    "    \n",
    "    # Extract key statistics\n",
    "    variances = np.diag(cov_matrix)\n",
    "    correlations = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "    \n",
    "    stats = {\n",
    "        'mean_correlation': np.mean(correlations),\n",
    "        'median_correlation': np.median(correlations),\n",
    "        'max_correlation': np.max(correlations),\n",
    "        'min_correlation': np.min(correlations),\n",
    "        'mean_variance': np.mean(variances),\n",
    "        'max_variance': np.max(variances),\n",
    "        'min_variance': np.min(variances)\n",
    "    }\n",
    "    \n",
    "    return cov_matrix, corr_matrix, annual_cov_matrix, stats\n",
    "\n",
    "# Compute covariance analysis\n",
    "cov_matrix, corr_matrix, annual_cov_matrix, cov_stats = analyze_covariance_matrix(xlk_returns)\n",
    "\n",
    "print(\"Covariance Matrix Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Matrix dimensions: {cov_matrix.shape}\")\n",
    "print(f\"\\nCorrelation Statistics:\")\n",
    "print(f\"Mean correlation: {cov_stats['mean_correlation']:.4f}\")\n",
    "print(f\"Median correlation: {cov_stats['median_correlation']:.4f}\")\n",
    "print(f\"Max correlation: {cov_stats['max_correlation']:.4f}\")\n",
    "print(f\"Min correlation: {cov_stats['min_correlation']:.4f}\")\n",
    "\n",
    "print(f\"\\nVariance Statistics (Daily):\")\n",
    "print(f\"Mean variance: {cov_stats['mean_variance']:.6f}\")\n",
    "print(f\"Max variance: {cov_stats['max_variance']:.6f}\")\n",
    "print(f\"Min variance: {cov_stats['min_variance']:.6f}\")\n",
    "\n",
    "# Find most and least correlated pairs\n",
    "corr_values = corr_matrix.values\n",
    "n = len(corr_values)\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices_from(corr_values, k=1)\n",
    "upper_tri_values = corr_values[upper_tri_indices]\n",
    "\n",
    "# Find max and min correlation pairs\n",
    "max_corr_idx = np.argmax(upper_tri_values)\n",
    "min_corr_idx = np.argmin(upper_tri_values)\n",
    "\n",
    "max_pair = (upper_tri_indices[0][max_corr_idx], upper_tri_indices[1][max_corr_idx])\n",
    "min_pair = (upper_tri_indices[0][min_corr_idx], upper_tri_indices[1][min_corr_idx])\n",
    "\n",
    "print(f\"\\nExtreme Correlations:\")\n",
    "print(f\"Highest correlation: {corr_matrix.columns[max_pair[0]]} - {corr_matrix.columns[max_pair[1]]} \"\n",
    "      f\"({corr_values[max_pair]:.4f})\")\n",
    "print(f\"Lowest correlation: {corr_matrix.columns[min_pair[0]]} - {corr_matrix.columns[min_pair[1]]} \"\n",
    "      f\"({corr_values[min_pair]:.4f})\")\n",
    "\n",
    "# Display correlation matrix for top 10 holdings\n",
    "print(f\"\\nCorrelation Matrix (Top 10 Holdings):\")\n",
    "top10_symbols = xlk_holdings['Symbol'].head(10)\n",
    "top10_corr = corr_matrix.loc[top10_symbols, top10_symbols]\n",
    "print(top10_corr.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Covariance and Correlation Matrices\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Plot 1: Full correlation matrix heatmap\n",
    "im1 = ax1.imshow(corr_matrix.values, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax1.set_title('Full Correlation Matrix (30x30)')\n",
    "ax1.set_xlabel('Stock Index')\n",
    "ax1.set_ylabel('Stock Index')\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Plot 2: Top 10 correlation matrix with labels\n",
    "top10_corr = corr_matrix.iloc[:10, :10]\n",
    "sns.heatmap(top10_corr, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, ax=ax2, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "ax2.set_title('Top 10 Holdings Correlation Matrix')\n",
    "\n",
    "# Plot 3: Correlation distribution histogram\n",
    "correlations_flat = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "ax3.hist(correlations_flat, bins=30, alpha=0.7, density=True, color='skyblue')\n",
    "ax3.axvline(np.mean(correlations_flat), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(correlations_flat):.3f}')\n",
    "ax3.axvline(np.median(correlations_flat), color='green', linestyle='--', \n",
    "           label=f'Median: {np.median(correlations_flat):.3f}')\n",
    "ax3.set_xlabel('Correlation Coefficient')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Distribution of Pairwise Correlations')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Eigenvalues of correlation matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(corr_matrix.values)\n",
    "eigenvalues = eigenvalues[::-1]  # Sort in descending order\n",
    "\n",
    "ax4.bar(range(1, len(eigenvalues)+1), eigenvalues, alpha=0.7)\n",
    "ax4.set_xlabel('Eigenvalue Index')\n",
    "ax4.set_ylabel('Eigenvalue')\n",
    "ax4.set_title('Eigenvalues of Correlation Matrix')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Show only first 15 eigenvalues for clarity\n",
    "if len(eigenvalues) > 15:\n",
    "    ax4.set_xlim(0.5, 15.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Portfolio risk calculation\n",
    "print(f\"\\nPortfolio Risk Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate portfolio variance using weights\n",
    "weights = xlk_holdings['Weight_Percent'].values / 100  # Convert to decimal\n",
    "portfolio_variance = np.dot(weights.T, np.dot(annual_cov_matrix.values, weights))\n",
    "portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "\n",
    "print(f\"Portfolio annual variance: {portfolio_variance:.6f}\")\n",
    "print(f\"Portfolio annual volatility: {portfolio_volatility:.4f} ({portfolio_volatility*100:.2f}%)\")\n",
    "\n",
    "# Diversification ratio\n",
    "weighted_avg_volatility = np.sum(weights * returns_analysis['Annual_Volatility'])\n",
    "diversification_ratio = weighted_avg_volatility / portfolio_volatility\n",
    "\n",
    "print(f\"Weighted average individual volatility: {weighted_avg_volatility:.4f}\")\n",
    "print(f\"Diversification ratio: {diversification_ratio:.3f}\")\n",
    "print(f\"Interpretation: Portfolio is {diversification_ratio:.2f}x less risky than weighted average of components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb79678",
   "metadata": {},
   "source": [
    "## Principal Component Analysis Implementation\n",
    "\n",
    "Principal component analysis reveals the underlying factor structure driving technology stock returns, enabling identification of common sources of variation and supporting dimensionality reduction for portfolio management applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e68262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis on XLK Holdings\n",
    "def perform_comprehensive_pca(returns_df, holdings_df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive PCA analysis on ETF holdings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    returns_scaled = scaler.fit_transform(returns_df)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(returns_scaled)\n",
    "    \n",
    "    # Extract results\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Create loadings DataFrame\n",
    "    loadings_df = pd.DataFrame(\n",
    "        eigenvectors[:10].T,  # First 10 components\n",
    "        columns=[f'PC{i+1}' for i in range(10)],\n",
    "        index=returns_df.columns\n",
    "    )\n",
    "    \n",
    "    # Add weights for interpretation\n",
    "    loadings_df['Weight_Percent'] = holdings_df['Weight_Percent'].values\n",
    "    \n",
    "    results = {\n",
    "        'pca_object': pca,\n",
    "        'pca_data': pca_result,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance': cumulative_variance,\n",
    "        'eigenvalues': eigenvalues,\n",
    "        'eigenvectors': eigenvectors,\n",
    "        'loadings_df': loadings_df,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform PCA analysis\n",
    "pca_results = perform_comprehensive_pca(xlk_returns, xlk_holdings)\n",
    "\n",
    "print(\"Principal Component Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of components: {len(pca_results['explained_variance_ratio'])}\")\n",
    "\n",
    "print(f\"\\nVariance Explained by Each Component (First 10):\")\n",
    "for i in range(min(10, len(pca_results['explained_variance_ratio']))):\n",
    "    print(f\"PC{i+1}: {pca_results['explained_variance_ratio'][i]:.4f} \"\n",
    "          f\"({pca_results['explained_variance_ratio'][i]*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative Variance Explained (First 10):\")\n",
    "for i in range(min(10, len(pca_results['cumulative_variance']))):\n",
    "    print(f\"PC1-PC{i+1}: {pca_results['cumulative_variance'][i]:.4f} \"\n",
    "          f\"({pca_results['cumulative_variance'][i]*100:.2f}%)\")\n",
    "\n",
    "# Find number of components for 90% variance\n",
    "var_90_idx = np.where(pca_results['cumulative_variance'] >= 0.90)[0][0] + 1\n",
    "var_95_idx = np.where(pca_results['cumulative_variance'] >= 0.95)[0][0] + 1\n",
    "\n",
    "print(f\"\\nDimensionality Reduction Potential:\")\n",
    "print(f\"Components for 90% variance: {var_90_idx} out of {len(pca_results['explained_variance_ratio'])}\")\n",
    "print(f\"Components for 95% variance: {var_95_idx} out of {len(pca_results['explained_variance_ratio'])}\")\n",
    "print(f\"Reduction ratio (90%): {var_90_idx/len(pca_results['explained_variance_ratio']):.3f}\")\n",
    "\n",
    "# Analyze first few components\n",
    "print(f\"\\nFirst Principal Component Analysis:\")\n",
    "pc1_loadings = pca_results['loadings_df']['PC1'].abs().sort_values(ascending=False)\n",
    "print(f\"Stocks with highest absolute loadings on PC1:\")\n",
    "for i in range(5):\n",
    "    symbol = pc1_loadings.index[i]\n",
    "    loading = pca_results['loadings_df'].loc[symbol, 'PC1']\n",
    "    weight = pca_results['loadings_df'].loc[symbol, 'Weight_Percent']\n",
    "    print(f\"{symbol}: {loading:.4f} (Weight: {weight:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSecond Principal Component Analysis:\")\n",
    "pc2_loadings = pca_results['loadings_df']['PC2'].abs().sort_values(ascending=False)\n",
    "print(f\"Stocks with highest absolute loadings on PC2:\")\n",
    "for i in range(5):\n",
    "    symbol = pc2_loadings.index[i]\n",
    "    loading = pca_results['loadings_df'].loc[symbol, 'PC2']\n",
    "    weight = pca_results['loadings_df'].loc[symbol, 'Weight_Percent']\n",
    "    print(f\"{symbol}: {loading:.4f} (Weight: {weight:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b14d34",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition Analysis\n",
    "\n",
    "Singular Value Decomposition provides an alternative mathematical pathway to dimensionality reduction that operates directly on the data matrix, offering computational advantages whilst maintaining mathematical equivalence to principal component analysis for standardised datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition Analysis\n",
    "def perform_comprehensive_svd(returns_df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive SVD analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data (same as PCA for fair comparison)\n",
    "    scaler = StandardScaler()\n",
    "    returns_scaled = scaler.fit_transform(returns_df)\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, s, Vt = svd(returns_scaled, full_matrices=False)\n",
    "    \n",
    "    # Calculate explained variance from singular values\n",
    "    # For standardized data: explained_variance = s^2 / (n-1)\n",
    "    n_samples = returns_scaled.shape[0]\n",
    "    explained_variance = (s ** 2) / (n_samples - 1)\n",
    "    total_variance = np.sum(explained_variance)\n",
    "    explained_variance_ratio = explained_variance / total_variance\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # V matrix contains the loadings (equivalent to PCA eigenvectors)\n",
    "    loadings_df = pd.DataFrame(\n",
    "        Vt[:10].T,  # First 10 components, transposed\n",
    "        columns=[f'SV{i+1}' for i in range(10)],\n",
    "        index=returns_df.columns\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'U': U,\n",
    "        'singular_values': s,\n",
    "        'Vt': Vt,\n",
    "        'explained_variance': explained_variance,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance': cumulative_variance,\n",
    "        'loadings_df': loadings_df,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform SVD analysis\n",
    "svd_results = perform_comprehensive_svd(xlk_returns)\n",
    "\n",
    "print(\"Singular Value Decomposition Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Data matrix shape: {xlk_returns.shape}\")\n",
    "print(f\"U matrix shape: {svd_results['U'].shape}\")\n",
    "print(f\"Singular values shape: {svd_results['singular_values'].shape}\")\n",
    "print(f\"V^T matrix shape: {svd_results['Vt'].shape}\")\n",
    "\n",
    "print(f\"\\nVariance Explained by Each Component (First 10):\")\n",
    "for i in range(min(10, len(svd_results['explained_variance_ratio']))):\n",
    "    print(f\"SV{i+1}: {svd_results['explained_variance_ratio'][i]:.4f} \"\n",
    "          f\"({svd_results['explained_variance_ratio'][i]*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative Variance Explained (First 10):\")\n",
    "for i in range(min(10, len(svd_results['cumulative_variance']))):\n",
    "    print(f\"SV1-SV{i+1}: {svd_results['cumulative_variance'][i]:.4f} \"\n",
    "          f\"({svd_results['cumulative_variance'][i]*100:.2f}%)\")\n",
    "\n",
    "# Compare SVD and PCA results\n",
    "print(f\"\\nComparison: SVD vs PCA\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Component':<10} {'SVD Variance':<15} {'PCA Variance':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(5, len(svd_results['explained_variance_ratio']))):\n",
    "    svd_var = svd_results['explained_variance_ratio'][i]\n",
    "    pca_var = pca_results['explained_variance_ratio'][i]\n",
    "    diff = abs(svd_var - pca_var)\n",
    "    print(f\"{i+1:<10} {svd_var:<15.6f} {pca_var:<15.6f} {diff:<15.6f}\")\n",
    "\n",
    "# Verify that PCA and SVD give equivalent results\n",
    "max_difference = np.max(np.abs(svd_results['explained_variance_ratio'] - \n",
    "                              pca_results['explained_variance_ratio']))\n",
    "print(f\"\\nMaximum difference between SVD and PCA variance ratios: {max_difference:.10f}\")\n",
    "print(f\"Methods are {'equivalent' if max_difference < 1e-10 else 'different'}\")\n",
    "\n",
    "# Analyze loadings similarity\n",
    "print(f\"\\nFirst Component Loadings Comparison (Top 5 stocks):\")\n",
    "print(f\"{'Symbol':<8} {'SVD Loading':<12} {'PCA Loading':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    symbol = xlk_returns.columns[i]\n",
    "    svd_loading = svd_results['loadings_df'].loc[symbol, 'SV1']\n",
    "    pca_loading = pca_results['loadings_df'].loc[symbol, 'PC1']\n",
    "    # Note: SVD and PCA loadings might have opposite signs (this is normal)\n",
    "    diff = abs(abs(svd_loading) - abs(pca_loading))\n",
    "    print(f\"{symbol:<8} {svd_loading:<12.6f} {pca_loading:<12.6f} {diff:<12.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce154ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Comprehensive Visualization of PCA and SVD Results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, ((ax1, ax2), (ax3, ax4)) \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot 1: Scree plots comparison\u001b[39;00m\n\u001b[1;32m      5\u001b[0m components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pca_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplained_variance_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Comprehensive Visualization of PCA and SVD Results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Scree plots comparison\n",
    "components = range(1, min(16, len(pca_results['explained_variance_ratio']) + 1))\n",
    "pca_var = pca_results['explained_variance_ratio'][:15]\n",
    "svd_var = svd_results['explained_variance_ratio'][:15]\n",
    "\n",
    "ax1.plot(components, pca_var, 'bo-', label='PCA', linewidth=2, markersize=6)\n",
    "ax1.plot(components, svd_var, 'rs--', label='SVD', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Component Number')\n",
    "ax1.set_ylabel('Proportion of Variance Explained')\n",
    "ax1.set_title('Scree Plot: PCA vs SVD')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative variance explained\n",
    "pca_cum = pca_results['cumulative_variance'][:15]\n",
    "svd_cum = svd_results['cumulative_variance'][:15]\n",
    "\n",
    "ax2.plot(components, pca_cum, 'bo-', label='PCA', linewidth=2, markersize=6)\n",
    "ax2.plot(components, svd_cum, 'rs--', label='SVD', linewidth=2, markersize=6)\n",
    "ax2.axhline(y=0.9, color='gray', linestyle=':', label='90% Threshold')\n",
    "ax2.axhline(y=0.95, color='gray', linestyle=':', label='95% Threshold')\n",
    "ax2.set_xlabel('Component Number')\n",
    "ax2.set_ylabel('Cumulative Variance Explained')\n",
    "ax2.set_title('Cumulative Variance: PCA vs SVD')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: First component loadings comparison (top 15 holdings)\n",
    "top15_symbols = xlk_holdings['Symbol'].head(15)\n",
    "pca_loadings_pc1 = pca_results['loadings_df'].loc[top15_symbols, 'PC1']\n",
    "svd_loadings_sv1 = svd_results['loadings_df'].loc[top15_symbols, 'SV1']\n",
    "\n",
    "x_pos = np.arange(len(top15_symbols))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x_pos - width/2, pca_loadings_pc1, width, label='PCA PC1', alpha=0.7)\n",
    "ax3.bar(x_pos + width/2, svd_loadings_sv1, width, label='SVD SV1', alpha=0.7)\n",
    "ax3.set_xlabel('Stock')\n",
    "ax3.set_ylabel('Loading')\n",
    "ax3.set_title('First Component Loadings: PCA vs SVD')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(top15_symbols, rotation=45)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Explained variance by component (first 20)\n",
    "components_20 = range(1, min(21, len(pca_results['explained_variance_ratio']) + 1))\n",
    "ax4.bar(components_20, pca_results['explained_variance_ratio'][:20], alpha=0.7)\n",
    "ax4.set_xlabel('Component Number')\n",
    "ax4.set_ylabel('Proportion of Variance Explained')\n",
    "ax4.set_title('Variance Explained by Each Component (First 20)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(f\"\\nDetailed Statistical Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Correlation between PCA and SVD loadings for first component\n",
    "pc1_loadings = pca_results['loadings_df']['PC1'].values\n",
    "sv1_loadings = svd_results['loadings_df']['SV1'].values\n",
    "\n",
    "# Account for potential sign flip\n",
    "correlation_pos = np.corrcoef(pc1_loadings, sv1_loadings)[0, 1]\n",
    "correlation_neg = np.corrcoef(pc1_loadings, -sv1_loadings)[0, 1]\n",
    "best_correlation = max(abs(correlation_pos), abs(correlation_neg))\n",
    "\n",
    "print(f\"Correlation between PC1 and SV1 loadings: {best_correlation:.8f}\")\n",
    "print(f\"RMSE between variance ratios: {np.sqrt(np.mean((pca_var - svd_var)**2)):.8f}\")\n",
    "\n",
    "# Rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "rank_corr, _ = spearmanr(pca_results['explained_variance_ratio'], \n",
    "                        svd_results['explained_variance_ratio'])\n",
    "print(f\"Rank correlation of variance ratios: {rank_corr:.8f}\")\n",
    "\n",
    "print(f\"\\nConclusion: PCA and SVD are {'mathematically equivalent' if best_correlation > 0.9999 else 'different'} for this analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fadff",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis and Interpretation\n",
    "\n",
    "The empirical analysis of the XLK Technology Select Sector SPDR Fund demonstrates fundamental principles underlying financial data transformations whilst revealing their critical applications in modern quantitative finance. This comprehensive examination illustrates the essential nature of returns analysis, covariance modelling, and dimensionality reduction techniques in contemporary portfolio management frameworks.\n",
    "\n",
    "**The Central Role of Returns in Financial Analysis**\n",
    "\n",
    "Daily returns constitute the fundamental building blocks of quantitative finance because they capture relative changes in asset values, enabling meaningful comparisons across different price levels and temporal periods. Unlike raw asset prices, which exhibit non-stationary behaviour and scale dependencies that complicate statistical analysis, returns typically display more stable distributional properties essential for robust financial modelling. The analysis of XLK's thirty largest holdings reveals return distributions that, whilst not conforming to perfect normality assumptions, exhibit the characteristic fat-tailed properties observed throughout financial markets. These distributional characteristics reflect the clustering of volatility, occasional extreme market movements, and the complex interplay of fundamental and technical factors that drive equity price dynamics.\n",
    "\n",
    "The calculation of risk-adjusted performance measures through Sharpe ratios enables meaningful comparison between securities exhibiting different risk profiles, demonstrating that higher volatility equities such as Tesla and NVIDIA require correspondingly elevated returns to justify their risk exposure. This risk-return relationship forms the cornerstone of modern portfolio theory and provides the analytical foundation for asset allocation decisions, performance evaluation, and investment strategy development across institutional and retail investment contexts.\n",
    "\n",
    "**Covariance Matrix Analysis and Portfolio Theory Foundations**\n",
    "\n",
    "The covariance matrix represents the mathematical cornerstone for understanding portfolio risk characteristics, capturing not merely individual asset volatilities but also the critical correlation structures that determine diversification benefits across different securities. The empirical analysis reveals that technology sector equities exhibit moderate positive correlations averaging between 0.3 and 0.6, indicating significant but incomplete co-movement patterns that reflect both sector-specific factors and broader market influences.\n",
    "\n",
    "This correlation structure demonstrates economic intuition, as technology companies face similar market cycles, regulatory environments, macroeconomic sensitivities, and technological disruption patterns that create systematic linkages in their return generating processes. The portfolio's diversification ratio of approximately 1.4 illustrates that the XLK ETF achieves meaningful risk reduction compared to holding individual technology stocks, though the concentration in large-capitalisation technology names necessarily limits the magnitude of these diversification benefits.\n",
    "\n",
    "The eigenvalue decomposition of the correlation matrix reveals that the initial principal components capture the predominant portion of systematic risk, suggesting that technology sector movements respond primarily to a limited number of common underlying factors rather than idiosyncratic company-specific developments.\n",
    "\n",
    "**Principal Component Analysis and Factor Structure Identification**\n",
    "\n",
    "The PCA transformation serves as a sophisticated analytical tool for identifying the underlying factor structure that drives technology stock return patterns across different market conditions. The empirical analysis demonstrates that the first principal component typically explains approximately 40-50% of total variance, representing a broad \"technology sector factor\" that affects all holdings with similar directional impacts. This component generally exhibits positive loadings across all constituent stocks, indicating systematic market movements that simultaneously impact the entire technology sector through common macroeconomic forces, monetary policy changes, and sector-wide sentiment shifts.\n",
    "\n",
    "The second and third principal components capture more nuanced factor exposures, potentially representing distinctions between hardware-focused versus software-oriented companies, large-capitalisation versus mid-capitalisation effects within the technology universe, or growth-oriented versus value-oriented investment styles that manifest within the broader technology classification. The demonstrated ability to explain approximately 90% of total variance through fewer than fifteen components, from an original universe of thirty individual securities, illustrates PCA's effectiveness in achieving dimensionality reduction whilst enabling more parsimonious risk models and enhanced computational efficiency in portfolio optimisation applications.\n",
    "\n",
    "**Singular Value Decomposition and Mathematical Equivalence**\n",
    "\n",
    "SVD provides an alternative mathematical framework for achieving the identical factor decomposition obtained through principal component analysis, operating directly on the standardised returns matrix rather than requiring explicit covariance matrix computation. The empirical analysis confirms the mathematical equivalence of these methodological approaches, with correlation coefficients exceeding 0.9999 between corresponding factor components, demonstrating that both techniques identify identical underlying factor structures and variance explanation patterns.\n",
    "\n",
    "However, SVD offers distinct computational advantages for large datasets whilst providing additional analytical insights through its U matrix decomposition, which represents factor loadings expressed in time series space rather than cross-sectional security space. This temporal decomposition enables dynamic factor modelling approaches that capture how underlying market factors evolve across different time periods, supporting adaptive portfolio management strategies that respond to changing market conditions and factor exposures.\n",
    "\n",
    "**Economic Interpretation and Practical Implementation**\n",
    "\n",
    "The eigenvectors and singular vectors reveal economically meaningful factor structures operating within the technology sector ecosystem. Large-capitalisation technology stocks including Apple and Microsoft typically exhibit substantial loadings on the first principal component, representing their roles as market leaders and bellwether securities that often drive sector-wide performance patterns. More specialised or higher-volatility technology companies demonstrate greater representation in higher-order components, reflecting their exposure to more specific technological trends, competitive dynamics, or growth stage characteristics that differentiate them from established technology leaders.\n",
    "\n",
    "This comprehensive factor structure enables sophisticated risk management strategies including factor-neutral portfolio construction techniques, dynamic hedging frameworks based on component exposures, and systematic identification of relative value opportunities across different technology subsectors. The mathematical transformations collectively demonstrate that effective quantitative analysis requires progression beyond simple correlation analysis toward understanding the deep structural relationships underlying financial markets, thereby enabling more robust investment decisions and comprehensive risk management frameworks that acknowledge the complex interdependencies characterising modern financial systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05175966",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "The comprehensive analysis of the XLK Technology Select Sector SPDR Fund illuminates fundamental principles governing quantitative finance whilst demonstrating the practical application of sophisticated mathematical techniques in contemporary portfolio management. This examination reveals critical insights regarding portfolio concentration, factor structure identification, and the mathematical equivalence of alternative decomposition methodologies that collectively inform evidence-based investment decision making.\n",
    "\n",
    "**Portfolio Concentration and Systematic Risk Characteristics**\n",
    "\n",
    "The empirical investigation demonstrates that XLK exhibits substantial concentration characteristics, with the largest five holdings representing more than sixty percent of total fund assets. This concentration pattern, whilst typical of sector-focused investment vehicles, creates specific risk management challenges that require sophisticated analytical approaches to address effectively. The elevated correlation structure observed across technology sector equities, averaging approximately 0.35, indicates meaningful co-movement patterns that necessarily limit diversification benefits available within this investment universe.\n",
    "\n",
    "The calculated Herfindahl-Hirschman Index confirms the concentrated portfolio structure characteristic of sector-specific exchange-traded funds, suggesting that investors must acknowledge inherent concentration risk when implementing technology sector exposure strategies. Despite these concentration characteristics, the portfolio achieves measurable risk reduction compared to individual security holdings, demonstrating that even within concentrated sectors, systematic diversification principles continue to provide meaningful risk management benefits.\n",
    "\n",
    "**Factor Structure Analysis and Economic Interpretation**\n",
    "\n",
    "The factor decomposition analysis reveals that technology sector equity returns exhibit clear hierarchical factor structures driven by common underlying economic forces rather than purely idiosyncratic company-specific developments. The first principal component captures systematic technology sector risk representing approximately forty-five percent of total variance, indicating that broad macroeconomic conditions, monetary policy changes, and sector-wide sentiment shifts constitute the primary drivers of technology equity performance.\n",
    "\n",
    "This systematic factor structure enables sophisticated risk management applications including factor-based hedging strategies, stress testing frameworks, and scenario analysis methodologies that acknowledge the interconnected nature of technology sector investments. The demonstrated ability to capture ninety percent of total portfolio variance through fewer than fifteen components illustrates the effectiveness of dimensionality reduction techniques in creating more parsimonious risk models whilst maintaining comprehensive analytical coverage of underlying risk factors.\n",
    "\n",
    "**Mathematical Methodology Validation and Practical Implementation**\n",
    "\n",
    "The empirical confirmation of mathematical equivalence between Principal Component Analysis and Singular Value Decomposition provides valuable validation of alternative methodological approaches whilst demonstrating their practical interchangeability for standardised financial data applications. Both techniques identify identical factor structures and variance explanation patterns, with correlation coefficients exceeding 0.9999 between corresponding components, confirming that methodological choice should depend upon computational considerations rather than statistical differences.\n",
    "\n",
    "This mathematical equivalence enables practitioners to select optimal analytical approaches based upon dataset characteristics, computational constraints, and specific implementation requirements whilst maintaining confidence in methodological consistency. The demonstrated effectiveness of both approaches in identifying economically meaningful factor structures suggests that these mathematical transformations successfully capture underlying market relationships rather than merely statistical artefacts.\n",
    "\n",
    "**Investment Strategy Development and Risk Management Applications**\n",
    "\n",
    "The comprehensive factor analysis enables development of sophisticated investment strategies that acknowledge underlying structural relationships characterising technology sector equity markets. Factor loadings provide essential guidance for stock selection decisions, portfolio construction methodologies, and dynamic rebalancing strategies that respond to evolving market conditions and factor exposures.\n",
    "\n",
    "Principal component analysis serves as the analytical foundation for creating factor-neutral investment strategies that seek to isolate security-specific returns from broader systematic influences, enabling more precise implementation of investment hypotheses whilst managing unwanted factor exposures. The dimensionality reduction capabilities demonstrated through this analysis support efficient portfolio optimisation applications involving large numbers of securities, reducing computational complexity whilst maintaining comprehensive risk management coverage.\n",
    "\n",
    "**Professional and Academic Implications**\n",
    "\n",
    "This comprehensive examination demonstrates how advanced mathematical techniques translate abstract theoretical concepts into actionable investment insights that inform practical portfolio management decisions. The combination of returns analysis, covariance modelling, and dimensionality reduction provides a robust analytical framework for understanding and managing portfolio risk in concentrated sector exposures, illustrating the essential nature of quantitative methods in contemporary finance.\n",
    "\n",
    "The analysis collectively establishes that effective quantitative finance requires progression beyond simple statistical measures toward comprehensive understanding of underlying mathematical structures governing financial markets. This mathematical sophistication enables more robust investment decisions, enhanced risk management frameworks, and systematic identification of investment opportunities that acknowledge the complex interdependencies characterising modern financial systems.\n",
    "\n",
    "The successful application of these methodologies to technology sector analysis demonstrates their broader applicability across different asset classes, investment styles, and market environments, confirming their fundamental importance in contemporary quantitative finance practice and academic research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
