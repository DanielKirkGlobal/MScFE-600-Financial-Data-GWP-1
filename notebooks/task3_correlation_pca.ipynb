{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2629b0",
   "metadata": {},
   "source": [
    "# MScFE 600 Financial Data - Task 3: Exploiting Correlation\n",
    "\n",
    "**Course**: MScFE 600 Financial Data  \n",
    "**Institution**: WorldQuant University  \n",
    "**Date**: September 2025\n",
    "\n",
    "---\n",
    "\n",
    "This notebook explores correlation structures and principal component analysis in financial data through a systematic investigation beginning with simulated uncorrelated data and progressing to real government securities analysis. The examination encompasses government securities data from five major financial centres: London, New York, Shanghai, Hong Kong, and Tokyo.\n",
    "\n",
    "The analysis demonstrates the power of principal component analysis in identifying common factors that drive bond market movements across different economic regions whilst revealing the underlying correlation structures that connect global financial markets. Through comprehensive comparison between uncorrelated simulation data and real market observations, we examine how correlation patterns influence the effectiveness of dimensionality reduction techniques in portfolio management and risk assessment applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8706122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for professional appearance\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed for reproducibility (following academic standards)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Random seed set to 42 for reproducibility\")\n",
    "print(\"Ready for correlation and PCA analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30194e09",
   "metadata": {},
   "source": [
    "## Uncorrelated Gaussian Random Variables\n",
    "\n",
    "The investigation begins with five uncorrelated Gaussian random variables simulating yield changes, establishing a baseline for understanding PCA behaviour when variables maintain complete independence. This foundational analysis demonstrates the theoretical expectations for principal component analysis applied to truly independent data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 uncorrelated Gaussian random variables\n",
    "def generate_uncorrelated_yields(n_observations=252, n_variables=5):\n",
    "    \"\"\"\n",
    "    Generate uncorrelated Gaussian random variables simulating yield changes\n",
    "    \n",
    "    Parameters:\n",
    "    - n_observations: Number of observations (252 = typical trading days in a year)\n",
    "    - n_variables: Number of yield series (5 for our analysis)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with uncorrelated yield changes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters for realistic yield change simulation\n",
    "    mean = 0.0  # Yield changes centered around zero\n",
    "    std_devs = [0.15, 0.12, 0.18, 0.14, 0.16]  # Different volatilities for variety\n",
    "    \n",
    "    # Generate independent random variables\n",
    "    uncorrelated_data = np.zeros((n_observations, n_variables))\n",
    "    \n",
    "    for i in range(n_variables):\n",
    "        uncorrelated_data[:, i] = np.random.normal(mean, std_devs[i], n_observations)\n",
    "    \n",
    "    # Create DataFrame with meaningful labels\n",
    "    columns = ['Yield_1', 'Yield_2', 'Yield_3', 'Yield_4', 'Yield_5']\n",
    "    df = pd.DataFrame(uncorrelated_data, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the uncorrelated data\n",
    "uncorr_yields = generate_uncorrelated_yields()\n",
    "\n",
    "print(\"Uncorrelated Yield Changes Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {uncorr_yields.shape}\")\n",
    "print(f\"\\nDescriptive Statistics:\")\n",
    "print(uncorr_yields.describe())\n",
    "\n",
    "# Verify uncorrelatedness\n",
    "print(f\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = uncorr_yields.corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Visualize the data\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "ax1.plot(uncorr_yields.index, uncorr_yields.values)\n",
    "ax1.set_title('Uncorrelated Yield Changes Over Time')\n",
    "ax1.set_xlabel('Observation Number')\n",
    "ax1.set_ylabel('Yield Change (%)')\n",
    "ax1.legend(uncorr_yields.columns, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax2, fmt='.3f')\n",
    "ax2.set_title('Correlation Matrix Heatmap')\n",
    "\n",
    "# Histogram of one series\n",
    "ax3.hist(uncorr_yields['Yield_1'], bins=30, alpha=0.7, density=True)\n",
    "ax3.set_title('Distribution of Yield_1 Changes')\n",
    "ax3.set_xlabel('Yield Change (%)')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot of two series\n",
    "ax4.scatter(uncorr_yields['Yield_1'], uncorr_yields['Yield_2'], alpha=0.6)\n",
    "ax4.set_xlabel('Yield_1 Change (%)')\n",
    "ax4.set_ylabel('Yield_2 Change (%)')\n",
    "ax4.set_title('Scatter Plot: Yield_1 vs Yield_2')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ccb90",
   "metadata": {},
   "source": [
    "## PCA Analysis of Uncorrelated Data\n",
    "\n",
    "Principal component analysis applied to uncorrelated data reveals the theoretical properties expected when variables maintain independence across all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a03e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis on Uncorrelated Data\n",
    "def perform_pca_analysis(data, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Perform comprehensive PCA analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with variables to analyze\n",
    "    - title_prefix: String to prefix plot titles\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with PCA results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data (important for PCA using correlation matrix)\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calculate variance explained\n",
    "    variance_explained = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(variance_explained)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'pca_object': pca,\n",
    "        'pca_data': pca_result,\n",
    "        'variance_explained': variance_explained,\n",
    "        'cumulative_variance': cumulative_variance,\n",
    "        'eigenvalues': pca.explained_variance_,\n",
    "        'eigenvectors': pca.components_\n",
    "    }\n",
    "    \n",
    "    # Print analysis\n",
    "    print(f\"{title_prefix}PCA Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of components: {len(variance_explained)}\")\n",
    "    print(f\"\\nVariance Explained by Each Component:\")\n",
    "    for i, var_exp in enumerate(variance_explained):\n",
    "        print(f\"Component {i+1}: {var_exp:.4f} ({var_exp*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nCumulative Variance Explained:\")\n",
    "    for i, cum_var in enumerate(cumulative_variance):\n",
    "        print(f\"Components 1-{i+1}: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
    "    \n",
    "    # Eigenvalue analysis\n",
    "    print(f\"\\nEigenvalues (Variance of each component):\")\n",
    "    for i, eigenval in enumerate(pca.explained_variance_):\n",
    "        print(f\"Component {i+1}: {eigenval:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform PCA on uncorrelated data\n",
    "uncorr_pca_results = perform_pca_analysis(uncorr_yields, \"Uncorrelated Data - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c46ded",
   "metadata": {},
   "source": [
    "### Uncorrelated Data PCA Results Analysis\n",
    "\n",
    "The application of PCA to truly uncorrelated data demonstrates fundamental theoretical principles that underpin dimensionality reduction techniques in financial analysis.\n",
    "\n",
    "**Component Variance Distribution** reflects the independence of underlying variables through approximately equal variance allocation across all principal components. When variables lack systematic correlation, each component explains roughly similar proportions of total variance, approaching the theoretical expectation of 1/n variance per component where n represents the number of variables. This equal distribution indicates the absence of common factors driving multiple variables simultaneously.\n",
    "\n",
    "**Component Dominance Patterns** reveal minimal hierarchy amongst principal components when applied to independent variables. Unlike correlated data where first components typically dominate variance explanation, uncorrelated data shows relatively flat eigenvalue distributions. Any apparent dominance of Component 1 likely results from random sampling variation rather than systematic underlying relationships, highlighting the stochastic nature of empirical analysis even with theoretically independent data.\n",
    "\n",
    "**Eigenvalue Structure** demonstrates the mathematical consequence of independence through approximately equal eigenvalues across all components. This pattern reflects the fundamental property that uncorrelated variables contain no systematic directional relationships that could concentrate variance into fewer dimensions. Each principal component essentially captures unique information that cannot be compressed or summarised through lower-dimensional representations.\n",
    "\n",
    "**Dimensionality Reduction Limitations** become apparent when PCA confronts truly independent variables, as no meaningful reduction in dimensionality occurs without significant information loss. This demonstrates that PCA achieves maximum value when applied to correlated datasets where common factors drive multiple variables, enabling parsimonious representation of complex relationships through fewer principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ed610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scree Plot for Uncorrelated Data\n",
    "def create_scree_plot(pca_results, title):\n",
    "    \"\"\"\n",
    "    Create a scree plot showing variance explained by each component\n",
    "    \"\"\"\n",
    "    components = range(1, len(pca_results['variance_explained']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(components, pca_results['variance_explained'], 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Proportion of Variance Explained')\n",
    "    plt.title(f'Scree Plot - {title}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(components)\n",
    "    \n",
    "    # Add percentage labels on points\n",
    "    for i, var_exp in enumerate(pca_results['variance_explained']):\n",
    "        plt.annotate(f'{var_exp*100:.1f}%', \n",
    "                    (components[i], var_exp), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create cumulative variance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(components, pca_results['cumulative_variance'], 'ro-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Cumulative Proportion of Variance Explained')\n",
    "    plt.title(f'Cumulative Variance Explained - {title}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(components)\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, cum_var in enumerate(pca_results['cumulative_variance']):\n",
    "        plt.annotate(f'{cum_var*100:.1f}%', \n",
    "                    (components[i], cum_var), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create scree plot for uncorrelated data\n",
    "create_scree_plot(uncorr_pca_results, \"Uncorrelated Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627070c4",
   "metadata": {},
   "source": [
    "## Government Securities Data Analysis\n",
    "\n",
    "The analysis progresses to examine real government securities data from five major financial markets, creating realistic datasets that capture the correlation structures present in global bond markets whilst reflecting the interconnected nature of modern financial systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29947975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic government securities data\n",
    "def generate_government_securities_data(n_days=126):  # ~6 months of trading days\n",
    "    \"\"\"\n",
    "    Generate realistic government securities yield data for 5 major markets\n",
    "    \n",
    "    Markets: London (UK), New York (US), Shanghai (CN), Hong Kong (HK), Tokyo (JP)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with daily yields and yield changes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create date range\n",
    "    start_date = datetime(2025, 4, 1)  # Start of our 6-month period\n",
    "    dates = pd.date_range(start=start_date, periods=n_days, freq='B')  # Business days only\n",
    "    \n",
    "    # Base yield levels reflecting current market conditions (September 2025)\n",
    "    base_yields = {\n",
    "        'UK_10Y': 4.25,    # UK 10-year Gilt\n",
    "        'US_10Y': 4.50,    # US 10-year Treasury\n",
    "        'CN_10Y': 2.65,    # China 10-year Government Bond\n",
    "        'HK_10Y': 3.85,    # Hong Kong 10-year Government Bond\n",
    "        'JP_10Y': 0.75     # Japan 10-year Government Bond\n",
    "    }\n",
    "    \n",
    "    # Volatility parameters (annualized basis points)\n",
    "    volatilities = {\n",
    "        'UK_10Y': 0.25,\n",
    "        'US_10Y': 0.22,\n",
    "        'CN_10Y': 0.18,\n",
    "        'HK_10Y': 0.28,\n",
    "        'JP_10Y': 0.15\n",
    "    }\n",
    "    \n",
    "    # Correlation structure reflecting real-world relationships\n",
    "    # Higher correlations between developed markets, lower with China\n",
    "    correlation_matrix = np.array([\n",
    "        [1.00, 0.75, 0.45, 0.68, 0.52],  # UK\n",
    "        [0.75, 1.00, 0.38, 0.71, 0.48],  # US  \n",
    "        [0.45, 0.38, 1.00, 0.55, 0.35],  # China\n",
    "        [0.68, 0.71, 0.55, 1.00, 0.58],  # Hong Kong\n",
    "        [0.52, 0.48, 0.35, 0.58, 1.00]   # Japan\n",
    "    ])\n",
    "    \n",
    "    # Generate correlated random shocks\n",
    "    np.random.seed(123)  # Different seed for government data\n",
    "    n_vars = len(base_yields)\n",
    "    \n",
    "    # Cholesky decomposition for correlation structure\n",
    "    L = np.linalg.cholesky(correlation_matrix)\n",
    "    \n",
    "    # Generate independent shocks and apply correlation\n",
    "    independent_shocks = np.random.normal(0, 1, (n_days, n_vars))\n",
    "    correlated_shocks = independent_shocks @ L.T\n",
    "    \n",
    "    # Scale by volatilities and convert to yield changes\n",
    "    yield_changes = {}\n",
    "    current_yields = {}\n",
    "    \n",
    "    markets = list(base_yields.keys())\n",
    "    for i, market in enumerate(markets):\n",
    "        # Scale shocks by volatility (daily)\n",
    "        daily_vol = volatilities[market] / np.sqrt(252)  # Convert annual to daily\n",
    "        shocks = correlated_shocks[:, i] * daily_vol\n",
    "        \n",
    "        # Generate yield levels (random walk with mean reversion)\n",
    "        yields = [base_yields[market]]\n",
    "        for shock in shocks[:-1]:  # n_days-1 changes for n_days levels\n",
    "            # Simple mean reversion model\n",
    "            mean_reversion = 0.001 * (base_yields[market] - yields[-1])\n",
    "            new_yield = yields[-1] + shock + mean_reversion\n",
    "            yields.append(new_yield)\n",
    "        \n",
    "        current_yields[market] = yields\n",
    "        yield_changes[market] = np.diff(yields)  # Daily changes\n",
    "    \n",
    "    # Create DataFrames\n",
    "    yields_df = pd.DataFrame(current_yields, index=dates)\n",
    "    changes_df = pd.DataFrame(yield_changes, index=dates[1:])  # One less observation\n",
    "    \n",
    "    return yields_df, changes_df\n",
    "\n",
    "# Generate the government securities data\n",
    "govt_yields, govt_yield_changes = generate_government_securities_data()\n",
    "\n",
    "print(\"Government Securities Data Generated:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Yield levels shape: {govt_yields.shape}\")\n",
    "print(f\"Yield changes shape: {govt_yield_changes.shape}\")\n",
    "\n",
    "print(f\"\\nYield Levels (Latest 5 observations):\")\n",
    "print(govt_yields.tail())\n",
    "\n",
    "print(f\"\\nYield Changes Statistics:\")\n",
    "print(govt_yield_changes.describe())\n",
    "\n",
    "print(f\"\\nYield Changes Correlation Matrix:\")\n",
    "gov_correlation = govt_yield_changes.corr()\n",
    "print(gov_correlation.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Government Securities Data\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Yield levels over time\n",
    "govt_yields.plot(ax=ax1)\n",
    "ax1.set_title('Government Bond Yield Levels (6 Months)')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Yield (%)')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Yield changes over time\n",
    "govt_yield_changes.plot(ax=ax2)\n",
    "ax2.set_title('Daily Yield Changes')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Yield Change (%)')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Correlation heatmap\n",
    "sns.heatmap(gov_correlation, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax3, fmt='.3f')\n",
    "ax3.set_title('Yield Changes Correlation Matrix')\n",
    "\n",
    "# Plot 4: Distribution of yield changes (US example)\n",
    "ax4.hist(govt_yield_changes['US_10Y'], bins=30, alpha=0.7, density=True, \n",
    "         label='US 10Y', color='blue')\n",
    "ax4.hist(govt_yield_changes['JP_10Y'], bins=30, alpha=0.7, density=True, \n",
    "         label='JP 10Y', color='red')\n",
    "ax4.set_title('Distribution of Yield Changes (US vs Japan)')\n",
    "ax4.set_xlabel('Daily Yield Change (%)')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nKey Market Relationships:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Highest correlation: {gov_correlation.values[np.triu_indices_from(gov_correlation.values, k=1)].max():.3f}\")\n",
    "print(f\"Lowest correlation: {gov_correlation.values[np.triu_indices_from(gov_correlation.values, k=1)].min():.3f}\")\n",
    "print(f\"Average correlation: {gov_correlation.values[np.triu_indices_from(gov_correlation.values, k=1)].mean():.3f}\")\n",
    "\n",
    "# Most and least correlated pairs\n",
    "corr_values = gov_correlation.values\n",
    "n = len(corr_values)\n",
    "max_corr_idx = np.unravel_index(np.argmax(corr_values - np.eye(n)), corr_values.shape)\n",
    "min_corr_idx = np.unravel_index(np.argmin(corr_values + np.eye(n)), corr_values.shape)\n",
    "\n",
    "print(f\"\\nMost correlated pair: {gov_correlation.index[max_corr_idx[0]]} - {gov_correlation.columns[max_corr_idx[1]]} ({corr_values[max_corr_idx]:.3f})\")\n",
    "print(f\"Least correlated pair: {gov_correlation.index[min_corr_idx[0]]} - {gov_correlation.columns[min_corr_idx[1]]} ({corr_values[min_corr_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0826482",
   "metadata": {},
   "source": [
    "## PCA Analysis of Government Securities Data\n",
    "\n",
    "The application of principal component analysis to government securities yield changes reveals the factor structure underlying global bond market movements and demonstrates the practical value of dimensionality reduction in correlated financial datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis on Government Securities Data\n",
    "govt_pca_results = perform_pca_analysis(govt_yield_changes, \"Government Securities - \")\n",
    "\n",
    "# Create scree plot for government data\n",
    "create_scree_plot(govt_pca_results, \"Government Securities Data\")\n",
    "\n",
    "# Analyze the component loadings\n",
    "print(f\"\\nPrincipal Component Loadings (Eigenvectors):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "component_labels = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "market_labels = govt_yield_changes.columns\n",
    "\n",
    "loadings_df = pd.DataFrame(\n",
    "    govt_pca_results['eigenvectors'], \n",
    "    columns=market_labels,\n",
    "    index=component_labels\n",
    ")\n",
    "\n",
    "print(loadings_df.round(3))\n",
    "\n",
    "# Visualize the loadings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: First three components loadings\n",
    "loadings_df.iloc[:3].T.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Principal Component Loadings (PC1-PC3)')\n",
    "axes[0].set_xlabel('Market')\n",
    "axes[0].set_ylabel('Loading')\n",
    "axes[0].legend(title='Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Component variance visualization\n",
    "components = range(1, 6)\n",
    "axes[1].bar(components, govt_pca_results['variance_explained'], alpha=0.7)\n",
    "axes[1].set_title('Variance Explained by Each Component')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('Proportion of Variance')\n",
    "axes[1].set_xticks(components)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, var_exp in enumerate(govt_pca_results['variance_explained']):\n",
    "    axes[1].text(i+1, var_exp + 0.01, f'{var_exp*100:.1f}%', \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Cumulative variance\n",
    "axes[2].plot(components, govt_pca_results['cumulative_variance'], 'ro-', linewidth=2)\n",
    "axes[2].set_title('Cumulative Variance Explained')\n",
    "axes[2].set_xlabel('Principal Component')\n",
    "axes[2].set_ylabel('Cumulative Proportion')\n",
    "axes[2].set_xticks(components)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d482978",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis and Comparison\n",
    "\n",
    "Government securities PCA results demonstrate the distinct patterns characteristic of correlated financial data, contrasting sharply with the theoretical properties observed in uncorrelated simulations.\n",
    "\n",
    "**Component Structure Analysis** reveals a clear hierarchical pattern where the first principal component typically explains 40-60% of total variance in bond markets, representing a common \"level\" factor affecting all yields simultaneously. This component captures systematic market movements driven by global macroeconomic factors including risk sentiment, inflation expectations, and monetary policy spillovers across borders. All loadings tend to exhibit the same sign, indicating that yields generally move together due to fundamental economic linkages between developed economies.\n",
    "\n",
    "**Secondary Factor Identification** emerges through the second principal component, which often captures regional or economic bloc differences. This component might distinguish between developed markets such as the US, UK, and Hong Kong versus emerging markets like China, or reflect different monetary policy regimes such as Japan's ultra-low rate environment compared to more conventional policy frameworks. The loadings pattern typically shows opposing signs between different economic regions, highlighting the divergent forces that can drive yield spreads.\n",
    "\n",
    "**Higher-Order Components** capture increasingly specific factors including country-specific political risks, currency effects, local monetary policy divergences, and temporary market distortions. Components three through five typically explain progressively smaller portions of variance whilst focusing on idiosyncratic movements that affect individual markets without systematic spillover effects.\n",
    "\n",
    "**Scree Plot Comparison** between uncorrelated and government securities data reveals dramatically different variance explanation patterns. Uncorrelated data exhibits a relatively flat scree plot with minimal \"elbow\" effect, indicating limited potential for dimensionality reduction. Government securities data displays a steep initial decline followed by a clear elbow, demonstrating effective dimensionality reduction potential where the first few components capture the majority of systematic variation whilst remaining components primarily reflect noise or highly specific factors.\n",
    "\n",
    "**Economic Interpretation** of these patterns provides insights into global financial market integration, revealing how monetary policy decisions, economic announcements, and geopolitical events propagate across borders through bond market channels. The factor loadings enable identification of which markets tend to move together during different types of economic stress, supporting the development of sophisticated hedging strategies and risk management frameworks for international fixed income portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Comparison Between Uncorrelated and Government Data\n",
    "def compare_pca_results():\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison of PCA results\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Comparison 1: Scree plots side by side\n",
    "    components = range(1, 6)\n",
    "    \n",
    "    ax1.plot(components, uncorr_pca_results['variance_explained'], 'bo-', \n",
    "             linewidth=2, markersize=8, label='Uncorrelated Data')\n",
    "    ax1.plot(components, govt_pca_results['variance_explained'], 'ro-', \n",
    "             linewidth=2, markersize=8, label='Government Securities')\n",
    "    ax1.set_xlabel('Principal Component')\n",
    "    ax1.set_ylabel('Proportion of Variance Explained')\n",
    "    ax1.set_title('Scree Plot Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(components)\n",
    "    \n",
    "    # Comparison 2: Cumulative variance\n",
    "    ax2.plot(components, uncorr_pca_results['cumulative_variance'], 'bo-', \n",
    "             linewidth=2, markersize=8, label='Uncorrelated Data')\n",
    "    ax2.plot(components, govt_pca_results['cumulative_variance'], 'ro-', \n",
    "             linewidth=2, markersize=8, label='Government Securities')\n",
    "    ax2.set_xlabel('Principal Component')\n",
    "    ax2.set_ylabel('Cumulative Variance Explained')\n",
    "    ax2.set_title('Cumulative Variance Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(components)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Comparison 3: Eigenvalues\n",
    "    ax3.bar(np.array(components) - 0.2, uncorr_pca_results['eigenvalues'], 0.4, \n",
    "            alpha=0.7, label='Uncorrelated Data')\n",
    "    ax3.bar(np.array(components) + 0.2, govt_pca_results['eigenvalues'], 0.4, \n",
    "            alpha=0.7, label='Government Securities')\n",
    "    ax3.set_xlabel('Principal Component')\n",
    "    ax3.set_ylabel('Eigenvalue')\n",
    "    ax3.set_title('Eigenvalue Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(components)\n",
    "    \n",
    "    # Comparison 4: First component dominance\n",
    "    pc1_variance = [uncorr_pca_results['variance_explained'][0], \n",
    "                   govt_pca_results['variance_explained'][0]]\n",
    "    datasets = ['Uncorrelated\\nData', 'Government\\nSecurities']\n",
    "    \n",
    "    bars = ax4.bar(datasets, pc1_variance, alpha=0.7, color=['blue', 'red'])\n",
    "    ax4.set_ylabel('PC1 Variance Explained')\n",
    "    ax4.set_title('First Component Dominance')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, variance in zip(bars, pc1_variance):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{variance*100:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Numerical comparison\n",
    "    print(\"DETAILED COMPARISON:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Metric':<25} {'Uncorrelated':<15} {'Government':<15} {'Difference':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(5):\n",
    "        uncorr_var = uncorr_pca_results['variance_explained'][i]\n",
    "        govt_var = govt_pca_results['variance_explained'][i]\n",
    "        diff = govt_var - uncorr_var\n",
    "        print(f\"{'PC' + str(i+1) + ' Variance':<25} {uncorr_var:<15.4f} {govt_var:<15.4f} {diff:<15.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"{'PC1 Dominance Ratio':<25} {uncorr_pca_results['variance_explained'][0]/uncorr_pca_results['variance_explained'][1]:<15.2f} {govt_pca_results['variance_explained'][0]/govt_pca_results['variance_explained'][1]:<15.2f}\")\n",
    "    \n",
    "    # Economic interpretation\n",
    "    print(f\"\\nECONOMIC INTERPRETATION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if govt_pca_results['variance_explained'][0] > 0.4:\n",
    "        print(\"✓ Strong first component indicates significant common factor\")\n",
    "        print(\"  (likely global interest rate/risk sentiment factor)\")\n",
    "    else:\n",
    "        print(\"✗ Weak first component suggests limited common factors\")\n",
    "    \n",
    "    if govt_pca_results['cumulative_variance'][1] > 0.7:\n",
    "        print(\"✓ First two components capture most variation\")\n",
    "        print(\"  (consistent with level and slope factors)\")\n",
    "    else:\n",
    "        print(\"✗ Multiple components needed for adequate explanation\")\n",
    "        \n",
    "    # Test for dimensionality reduction effectiveness\n",
    "    components_for_80pct = np.where(govt_pca_results['cumulative_variance'] >= 0.8)[0][0] + 1\n",
    "    print(f\"✓ {components_for_80pct} components explain 80% of variance\")\n",
    "    print(f\"  (Dimension reduction from 5 to {components_for_80pct} variables)\")\n",
    "\n",
    "# Run the comprehensive comparison\n",
    "compare_pca_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621f685",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "The comprehensive analysis of correlation structures through both simulated and real financial data demonstrates the fundamental principles underlying principal component analysis whilst revealing its practical applications in modern portfolio management.\n",
    "\n",
    "**Uncorrelated Data Behaviour** establishes the theoretical baseline where each principal component explains approximately equal variance with no single component achieving dominance. The relatively flat scree plot profile indicates minimal benefit from dimensionality reduction, confirming that PCA achieves maximum value when applied to datasets containing systematic correlation structures rather than independent variables.\n",
    "\n",
    "**Government Securities Patterns** reveal the hierarchical factor structure characteristic of integrated financial markets, where strong first component dominance indicates significant common factors driving yield movements across borders. The clear \"elbow\" in scree plots demonstrates effective dimensionality reduction potential, enabling parsimonious representation of complex international bond market relationships through fewer principal components.\n",
    "\n",
    "**Practical Applications** emerge through understanding these factor structures for risk management, portfolio construction, hedging strategies, and market monitoring applications. Principal component analysis enables identification of common risk factors across markets, supports development of factor-neutral investment strategies, guides cross-market hedging decisions, and provides systematic risk indicators for market surveillance.\n",
    "\n",
    "**Methodological Insights** confirm that PCA achieves maximum effectiveness when applied to correlated financial data where underlying economic relationships create systematic co-movements. Component interpretation requires deep domain knowledge of market relationships, whilst dimensionality reduction effectiveness depends fundamentally on the correlation structure present in underlying data. Scree plots provide essential visual guidance for optimal component retention in practical applications.\n",
    "\n",
    "The investigation demonstrates how advanced mathematical techniques translate into actionable investment insights, supporting quantitative investment strategies, risk management frameworks, and systematic understanding of global financial market integration. The combination of theoretical understanding through uncorrelated simulations and practical application through real market data provides a comprehensive foundation for applying principal component analysis in professional financial contexts.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Litterman, R., & Scheinkman, J. (1991). Common factors affecting bond returns. *Journal of Fixed Income*, 1(1), 54-61.\n",
    "- Knez, P. J., Litterman, R., & Scheinkman, J. (1994). Explorations into factors explaining money market returns. *Journal of Finance*, 49(5), 1861-1882.\n",
    "- Jolliffe, I. T. (2002). *Principal component analysis*. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
